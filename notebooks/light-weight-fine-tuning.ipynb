{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc0551af-9054-43ca-8735-4d9da154e4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.11\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "<font color=\"red\"> \n",
    "\n",
    "- PEFT technique: LoRA (Low-Rank Adaptation)\n",
    "- Model: `distilbert-base-uncased`\n",
    "- Evaluation approach: Accuracy and F1-score\n",
    "- Fine-tuning dataset: IMDb Reviews (binary sentiment classification)\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "from peft import get_peft_model, LoraConfig, AutoPeftModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6d9857-214c-4659-bd72-67e62f73502f",
   "metadata": {},
   "source": [
    "## Helpful Functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1a0f630-66d8-4b91-9ba4-f1c5fc7c5f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(preds, labels)\n",
    "    f1 = f1_score(preds, labels, average=\"weighted\")\n",
    "    return {\"accuracy\": acc, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3e869e-4375-4026-932c-e85f2aae91a0",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained model and tokenizer\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f28c4a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "881ad130bda849438158e6a8752c0cc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(\"imdb\", )\n",
    "dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Ensure that the model receives inputs in the correct format for training and evaluation\n",
    "dataset.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"label\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ee6f61f-ab6a-43fe-aa2d-0c22c0f024ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 06:51]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for pretrained model: {'eval_loss': 0.6977689266204834, 'eval_accuracy': 0.4996, 'eval_f1': 0.66597346981213, 'eval_runtime': 412.746, 'eval_samples_per_second': 60.57, 'eval_steps_per_second': 7.571}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the pretrained model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation for pretrained model: {eval_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd2c36c-6f3e-41ed-98c2-163d58adc6fb",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PEFT model\n",
    "peft_config = LoraConfig(\n",
    "    task_type=\"SEQ_CLS\",\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_lin\", \"v_lin\"]\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9d2fcb3-467f-4a1b-ab8c-abc7b92a0eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,331,716 || all params: 67,694,596 || trainable%: 1.967241225577297\n"
     ]
    }
   ],
   "source": [
    "# Check trainable parameters of the PEFT model\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "894046c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15630' max='15630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15630/15630 3:48:41, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.257000</td>\n",
       "      <td>0.237407</td>\n",
       "      <td>0.903920</td>\n",
       "      <td>0.903998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.243400</td>\n",
       "      <td>0.218991</td>\n",
       "      <td>0.912760</td>\n",
       "      <td>0.912815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.225800</td>\n",
       "      <td>0.209126</td>\n",
       "      <td>0.918240</td>\n",
       "      <td>0.918241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.212900</td>\n",
       "      <td>0.217593</td>\n",
       "      <td>0.918920</td>\n",
       "      <td>0.918933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.200100</td>\n",
       "      <td>0.202390</td>\n",
       "      <td>0.923040</td>\n",
       "      <td>0.923040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.198300</td>\n",
       "      <td>0.213539</td>\n",
       "      <td>0.923320</td>\n",
       "      <td>0.923323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.186900</td>\n",
       "      <td>0.215877</td>\n",
       "      <td>0.922760</td>\n",
       "      <td>0.922769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.185900</td>\n",
       "      <td>0.208203</td>\n",
       "      <td>0.923920</td>\n",
       "      <td>0.923920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.182300</td>\n",
       "      <td>0.214719</td>\n",
       "      <td>0.924080</td>\n",
       "      <td>0.924084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.179000</td>\n",
       "      <td>0.216512</td>\n",
       "      <td>0.923840</td>\n",
       "      <td>0.923840</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15630, training_loss=0.2109471498661444, metrics={'train_runtime': 13721.6774, 'train_samples_per_second': 18.219, 'train_steps_per_second': 1.139, 'total_flos': 3.3684851712e+16, 'train_loss': 0.2109471498661444, 'epoch': 10.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\"\n",
    ")\n",
    "\n",
    "# Train the PEFT model\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa7fe003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model saved!\n"
     ]
    }
   ],
   "source": [
    "# Save fine-tuned model\n",
    "peft_model.save_pretrained(\"./peft_model\")\n",
    "print(\"Fine-tuned model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f86158-1a3e-49fa-8201-b5fcd6b5cb72",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load fine-tuned PEFT model\n",
    "peft_model = AutoPeftModelForSequenceClassification.from_pretrained(\"./peft_model/\")\n",
    "print(\"Fine-tuned model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc96905a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 07:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for fine-tuned model: {'eval_loss': 0.21651224792003632, 'eval_accuracy': 0.92384, 'eval_f1': 0.9238403821423334, 'eval_runtime': 425.7198, 'eval_samples_per_second': 58.724, 'eval_steps_per_second': 7.341}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the fine-tuned model\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "eval_results_peft = trainer.evaluate()\n",
    "print(f\"Evaluation for fine-tuned model: {eval_results_peft}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78140ff3-0b59-4a3e-8b40-daf3397e17ac",
   "metadata": {},
   "source": [
    "<font color=\"red\"> \n",
    "\n",
    "Fine-tuning with PEFT significantly improved model performance, reducing the evaluation loss from **0.698** to **0.217** while boosting accuracy from **49.96%** to **92.38%** and F1-score from **66.60%** to **92.38%**. These results indicate that fine-tuning effectively adapted the model to the IMDb sentiment classification task, leading to much better predictive capability while maintaining a similar evaluation runtime.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d739423-a0fb-4bac-b982-22a33a4b19c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
